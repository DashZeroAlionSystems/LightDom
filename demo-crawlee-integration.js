#!/usr/bin/env node

/**
 * Complete Crawlee Integration Demo
 * Demonstrates the full workflow: Campaign ‚Üí Crawlee ‚Üí Seeder ‚Üí Results
 */

import { Pool } from 'pg';
import CrawleeService from './services/crawlee-service.js';
import CrawleeCampaignIntegration from './services/crawlee-campaign-integration.js';
import CrawleeSeederIntegration from './services/crawlee-seeder-integration.js';

const db = new Pool({
  host: process.env.DB_HOST || 'localhost',
  port: process.env.DB_PORT || 5432,
  database: process.env.DB_NAME || 'dom_space_harvester',
  user: process.env.DB_USER || 'postgres',
  password: process.env.DB_PASSWORD || 'postgres',
});

async function demoIntegration() {
  console.log('üéØ Complete Crawlee Integration Demo\n');
  console.log('This demo shows how Crawlee integrates with:');
  console.log('  ‚Ä¢ Campaign system');
  console.log('  ‚Ä¢ Seeder service');
  console.log('  ‚Ä¢ Database persistence');
  console.log('  ‚Ä¢ Real-time data extraction\n');

  try {
    // Initialize services
    console.log('1. Initializing services...');
    const crawleeService = new CrawleeService(db);
    const campaignIntegration = new CrawleeCampaignIntegration(db, null);
    const seederIntegration = new CrawleeSeederIntegration(db);
    console.log('‚úÖ Services initialized\n');

    // Ensure schemas
    console.log('2. Ensuring database schemas...');
    await campaignIntegration.ensureCampaignSchema();
    await seederIntegration.ensureSeederSchema();
    console.log('‚úÖ Schemas ready\n');

    // ===== SCENARIO 1: Campaign-based Crawling =====
    console.log('üìã SCENARIO 1: Campaign-based Crawling\n');
    
    console.log('3. Creating campaign with Crawlee crawler...');
    const campaign = {
      id: `campaign_demo_${Date.now()}`,
      name: 'SEO Campaign Demo',
      description: 'Demo campaign for testing Crawlee integration',
      useCrawlee: true,
      crawlerType: 'cheerio',
      schema: {
        fields: {
          title: 'h1',
          description: 'meta[name="description"]',
          h1: 'h1',
          h2: 'h2'
        }
      },
      seeds: {
        urls: [
          'https://example.com',
          'https://example.com/about'
        ],
        include: ['*'],
        exclude: []
      },
      configuration: {
        maxDepth: 2,
        parallelCrawlers: 3,
        maxRequestsPerCrawl: 10
      }
    };

    // Create crawler for campaign
    const campaignCrawler = await campaignIntegration.createCrawlerForCampaign(campaign);
    console.log(`‚úÖ Campaign crawler created: ${campaignCrawler.id}`);
    console.log(`   Campaign ID: ${campaign.id}`);
    console.log(`   Crawler Type: ${campaignCrawler.type}`);
    console.log(`   Seeds Added: ${campaign.seeds.urls.length}\n`);

    // ===== SCENARIO 2: Seeder-based Continuous Crawling =====
    console.log('üì° SCENARIO 2: Seeder-based Continuous Crawling\n');

    console.log('4. Creating seeder service...');
    const seederServiceId = `seeder_demo_${Date.now()}`;
    await db.query(`
      INSERT INTO seeding_services (id, name, description, status)
      VALUES ($1, $2, $3, $4)
    `, [
      seederServiceId,
      'Demo Seeder Service',
      'Continuous URL seeding for demo',
      'active'
    ]);
    console.log(`‚úÖ Seeder service created: ${seederServiceId}\n`);

    console.log('5. Adding URLs to seeder queue...');
    await seederIntegration.addUrlsToSeeder(seederServiceId, [
      { url: 'https://example.com/products', priority: 10 },
      { url: 'https://example.com/services', priority: 8 },
      { url: 'https://example.com/blog', priority: 5 },
      { url: 'https://example.com/contact', priority: 3 }
    ]);
    console.log('‚úÖ URLs added to seeder queue\n');

    console.log('6. Creating seeded crawler...');
    const seededCrawler = await seederIntegration.createSeededCrawler({
      name: 'Continuous SEO Crawler',
      description: 'Crawler that pulls from seeder service',
      seeder_service_id: seederServiceId,
      crawlerType: 'cheerio',
      selectors: {
        title: 'title',
        h1: 'h1',
        meta_description: {
          selector: 'meta[name="description"]',
          attribute: 'content'
        }
      },
      batchSize: 5,
      pollInterval: 10000 // 10 seconds
    });
    console.log(`‚úÖ Seeded crawler created: ${seededCrawler.id}`);
    console.log(`   Linked to seeder: ${seederServiceId}`);
    console.log(`   Batch size: 5`);
    console.log(`   Poll interval: 10s\n`);

    // ===== SCENARIO 3: Monitoring and Stats =====
    console.log('üìä SCENARIO 3: Monitoring and Stats\n');

    console.log('7. Listing all crawlers...');
    const allCrawlers = await crawleeService.listCrawlers();
    console.log(`‚úÖ Total crawlers: ${allCrawlers.length}`);
    allCrawlers.forEach((c, i) => {
      console.log(`   ${i + 1}. ${c.name} (${c.type}) - ${c.status}`);
      if (c.campaign_id) console.log(`      ‚Üí Campaign: ${c.campaign_id}`);
      if (c.seeder_service_id) console.log(`      ‚Üí Seeder: ${c.seeder_service_id}`);
    });
    console.log('');

    console.log('8. Checking seeder stats...');
    const seederStats = await seederIntegration.getSeederStats(seederServiceId);
    console.log('‚úÖ Seeder queue status:');
    console.log(`   Pending: ${seederStats.pending}`);
    console.log(`   Processing: ${seederStats.processing}`);
    console.log(`   Completed: ${seederStats.completed}`);
    console.log(`   Failed: ${seederStats.failed}`);
    console.log(`   Total: ${seederStats.total}\n`);

    // ===== SCENARIO 4: Data Flow =====
    console.log('üîÑ SCENARIO 4: Data Flow Demonstration\n');

    console.log('9. Simulating crawler execution...');
    console.log('   In a real scenario:');
    console.log('   a) Crawler starts ‚Üí picks up seeds');
    console.log('   b) Visits URLs ‚Üí extracts data with selectors');
    console.log('   c) Saves to crawlee_results table');
    console.log('   d) Updates campaign analytics');
    console.log('   e) Marks seeds as completed in seeder\n');

    console.log('10. Database tables populated:');
    console.log('   ‚úì crawlee_crawlers - Crawler configurations');
    console.log('   ‚úì crawlee_crawler_seeds - URLs to crawl');
    console.log('   ‚úì crawlee_results - Extracted data (empty until crawling)');
    console.log('   ‚úì url_seeds - Seeder queue');
    console.log('   ‚úì seeding_services - Seeder configs');
    console.log('   ‚úì seo_campaigns - Campaign data\n');

    // ===== Summary =====
    console.log('üìù INTEGRATION SUMMARY\n');
    console.log('‚úÖ Campaign Integration:');
    console.log('   ‚Ä¢ Campaigns can use Crawlee for crawling');
    console.log('   ‚Ä¢ Crawler config derived from campaign');
    console.log('   ‚Ä¢ Results automatically update campaign analytics');
    console.log('   ‚Ä¢ Campaign status synced with crawler\n');

    console.log('‚úÖ Seeder Integration:');
    console.log('   ‚Ä¢ Continuous URL feeding to crawlers');
    console.log('   ‚Ä¢ Priority-based queue processing');
    console.log('   ‚Ä¢ Automatic status tracking');
    console.log('   ‚Ä¢ Batch processing for efficiency\n');

    console.log('‚úÖ Data Persistence:');
    console.log('   ‚Ä¢ All data saved to PostgreSQL');
    console.log('   ‚Ä¢ Historical stats snapshots');
    console.log('   ‚Ä¢ Detailed logging');
    console.log('   ‚Ä¢ Campaign analytics updated\n');

    console.log('üéØ NEXT STEPS\n');
    console.log('1. Start API server: npm run api');
    console.log('2. Start frontend: npm run dev');
    console.log('3. Navigate to /crawlee-manager');
    console.log('4. View and manage crawlers');
    console.log('5. Start crawlers to see real data extraction\n');

    console.log('üìö API ENDPOINTS\n');
    console.log('‚Ä¢ GET  /api/crawlee/crawlers - List all crawlers');
    console.log('‚Ä¢ POST /api/crawlee/crawlers - Create crawler');
    console.log('‚Ä¢ POST /api/crawlee/crawlers/:id/start - Start crawler');
    console.log('‚Ä¢ GET  /api/crawlee/crawlers/:id/results - Get results');
    console.log('‚Ä¢ GET  /api/crawlee/crawlers/:id/stats - Get statistics\n');

    console.log('üéâ Demo completed successfully!\n');
    console.log('The system is now ready for 24/7 SEO data mining.');
    console.log('All components are integrated and working together.\n');

    // Cleanup demo data (optional)
    console.log('11. Cleaning up demo data...');
    await crawleeService.deleteCrawler(campaignCrawler.id);
    await crawleeService.deleteCrawler(seededCrawler.id);
    await db.query('DELETE FROM seeding_services WHERE id = $1', [seederServiceId]);
    await db.query('DELETE FROM url_seeds WHERE seeder_service_id = $1', [seederServiceId]);
    console.log('‚úÖ Demo data cleaned up\n');

  } catch (error) {
    console.error('‚ùå Demo failed:', error.message);
    console.error(error);
  } finally {
    await db.end();
  }
}

// Run if called directly
if (import.meta.url === `file://${process.argv[1]}`) {
  demoIntegration();
}

export default demoIntegration;
