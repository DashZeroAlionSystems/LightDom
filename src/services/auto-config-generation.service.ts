/**
 * Auto-Configuration Generation Service
 * 
 * Automatically generates optimal configurations from patterns,
 * tests them through simulation, and iteratively improves them.
 */

import { Pool } from 'pg';

export interface AutoGeneratedConfig {
  config_id: string;
  workflow_type: string;
  generated_config: any;
  generation_method: 'pattern_based' | 'ml_optimized' | 'hybrid';
  source_patterns: string[];
  confidence_score: number;
  created_at: Date;
}

export interface ConfigTestResult {
  test_id: string;
  config_id: string;
  test_input: any;
  simulation_result: any;
  success: boolean;
  performance_metrics: any;
  tested_at: Date;
}

export class AutoConfigGenerationService {
  constructor(private pool: Pool) {}

  /**
   * Generate configuration from successful patterns
   */
  async generateConfigFromPatterns(
    workflowType: string,
    criteria: {
      min_success_rate?: number;
      max_cost?: number;
      preferred_patterns?: string[];
    } = {}
  ): Promise<AutoGeneratedConfig> {
    const { min_success_rate = 0.7, max_cost = Infinity, preferred_patterns = [] } = criteria;

    // Find successful patterns for this workflow type
    const patternsResult = await this.pool.query(
      `SELECT * FROM learning_patterns
       WHERE pattern_category = $1
       AND confidence_score >= $2
       AND (metadata->>'avg_cost')::numeric <= $3
       ORDER BY confidence_score DESC, usage_count DESC
       LIMIT 10`,
      [workflowType, min_success_rate, max_cost]
    );

    if (patternsResult.rows.length === 0) {
      throw new Error(`No suitable patterns found for workflow type: ${workflowType}`);
    }

    const patterns = patternsResult.rows;

    // Merge patterns to create optimal config
    const mergedConfig = this.mergePatternConfigs(patterns, preferred_patterns);

    // Calculate confidence based on pattern strengths
    const confidenceScore = patterns.reduce((sum, p) => sum + p.confidence_score, 0) / patterns.length;

    const result = await this.pool.query(
      `INSERT INTO auto_generated_configs 
       (workflow_type, generated_config, generation_method, source_patterns, confidence_score)
       VALUES ($1, $2, $3, $4, $5)
       RETURNING *`,
      [
        workflowType,
        mergedConfig,
        'pattern_based',
        patterns.map(p => p.pattern_id),
        confidenceScore
      ]
    );

    return result.rows[0];
  }

  /**
   * Merge multiple pattern configurations into one optimal config
   */
  private mergePatternConfigs(patterns: any[], preferredPatterns: string[]): any {
    const config: any = {};

    // Priority: preferred patterns first, then by confidence score
    const sortedPatterns = [...patterns].sort((a, b) => {
      const aPreferred = preferredPatterns.includes(a.pattern_id) ? 1 : 0;
      const bPreferred = preferredPatterns.includes(b.pattern_id) ? 1 : 0;
      
      if (aPreferred !== bPreferred) {
        return bPreferred - aPreferred;
      }
      
      return b.confidence_score - a.confidence_score;
    });

    // Merge configurations
    for (const pattern of sortedPatterns) {
      const patternConfig = pattern.pattern_data?.config || pattern.pattern_data || {};
      
      for (const [key, value] of Object.entries(patternConfig)) {
        if (!(key in config)) {
          config[key] = value;
        } else if (typeof value === 'object' && !Array.isArray(value)) {
          // Merge nested objects
          config[key] = { ...config[key], ...value };
        }
        // For conflicts, keep the first value (highest priority pattern)
      }
    }

    // Add metadata about source patterns
    config._generated_from = sortedPatterns.map(p => ({
      pattern_id: p.pattern_id,
      confidence: p.confidence_score,
      description: p.description
    }));

    return config;
  }

  /**
   * Test generated configuration through simulation
   */
  async testGeneratedConfig(
    configId: string,
    testInput: any
  ): Promise<ConfigTestResult> {
    const config = await this.getConfig(configId);

    // TODO: Integrate with workflow simulation service
    // For now, simulate the test
    const simulationResult = {
      estimated_duration_ms: Math.random() * 10000,
      estimated_cost: Math.random() * 100,
      success_probability: config.confidence_score,
      predicted_output: {
        status: 'success',
        data: 'simulated result'
      }
    };

    const success = simulationResult.success_probability > 0.7;

    const performanceMetrics = {
      execution_time: simulationResult.estimated_duration_ms,
      cost: simulationResult.estimated_cost,
      accuracy: simulationResult.success_probability,
      resource_usage: {
        cpu: Math.random() * 100,
        memory: Math.random() * 1024,
        network: Math.random() * 100
      }
    };

    const result = await this.pool.query(
      `INSERT INTO config_test_results
       (config_id, test_input, simulation_result, success, performance_metrics)
       VALUES ($1, $2, $3, $4, $5)
       RETURNING *`,
      [configId, testInput, simulationResult, success, performanceMetrics]
    );

    return result.rows[0];
  }

  /**
   * Improve configuration iteratively
   */
  async improveConfig(
    configId: string,
    improvementCriteria: {
      target_metric: 'execution_time' | 'cost' | 'accuracy' | 'cost_effectiveness';
      improvement_goal: number; // e.g., 0.2 for 20% improvement
      max_iterations?: number;
    }
  ): Promise<AutoGeneratedConfig> {
    const { target_metric, improvement_goal, max_iterations = 5 } = improvementCriteria;

    const originalConfig = await this.getConfig(configId);
    let currentConfig = { ...originalConfig.generated_config };
    let bestConfig = currentConfig;
    let bestScore = 0;

    for (let i = 0; i < max_iterations; i++) {
      // Apply optimization based on target metric
      const optimizedConfig = this.applyOptimization(currentConfig, target_metric, improvement_goal);

      // Test the optimized configuration
      const testResult = await this.testGeneratedConfig(
        configId,
        { iteration: i, optimization_target: target_metric }
      );

      // Calculate score based on target metric
      const score = this.calculateScore(testResult.performance_metrics, target_metric);

      if (score > bestScore) {
        bestScore = score;
        bestConfig = optimizedConfig;
      }

      currentConfig = optimizedConfig;
    }

    // Create new config with improvements
    const result = await this.pool.query(
      `INSERT INTO auto_generated_configs
       (workflow_type, generated_config, generation_method, source_patterns, confidence_score)
       VALUES ($1, $2, $3, $4, $5)
       RETURNING *`,
      [
        originalConfig.workflow_type,
        bestConfig,
        'ml_optimized',
        [configId], // Source is the original config
        originalConfig.confidence_score * (1 + improvement_goal)
      ]
    );

    return result.rows[0];
  }

  /**
   * Apply optimization to configuration
   */
  private applyOptimization(config: any, targetMetric: string, improvementGoal: number): any {
    const optimized = { ...config };

    switch (targetMetric) {
      case 'execution_time':
        // Increase parallelism, reduce iterations, etc.
        if (optimized.parallelism) {
          optimized.parallelism = Math.min(10, optimized.parallelism * (1 + improvementGoal));
        }
        if (optimized.timeout) {
          optimized.timeout = optimized.timeout * (1 - improvementGoal);
        }
        break;

      case 'cost':
        // Reduce resource usage
        if (optimized.max_workers) {
          optimized.max_workers = Math.max(1, Math.floor(optimized.max_workers * (1 - improvementGoal)));
        }
        if (optimized.cache_enabled === undefined) {
          optimized.cache_enabled = true;
        }
        break;

      case 'accuracy':
        // Increase validation, add checks
        if (optimized.validation_level) {
          optimized.validation_level = 'strict';
        }
        if (optimized.retry_count) {
          optimized.retry_count = Math.ceil(optimized.retry_count * (1 + improvementGoal));
        }
        break;

      case 'cost_effectiveness':
        // Balance cost and performance
        if (optimized.optimization_level) {
          optimized.optimization_level = 'balanced';
        }
        break;
    }

    optimized._optimized_for = targetMetric;
    optimized._improvement_goal = improvementGoal;

    return optimized;
  }

  /**
   * Calculate score for a configuration based on metrics
   */
  private calculateScore(metrics: any, targetMetric: string): number {
    switch (targetMetric) {
      case 'execution_time':
        return 10000 / (metrics.execution_time || 10000);
      
      case 'cost':
        return 100 / (metrics.cost || 100);
      
      case 'accuracy':
        return metrics.accuracy || 0;
      
      case 'cost_effectiveness':
        const time = metrics.execution_time || 10000;
        const cost = metrics.cost || 100;
        const accuracy = metrics.accuracy || 0.5;
        return (accuracy * 100) / (time * cost / 1000000);
      
      default:
        return 0;
    }
  }

  /**
   * Get best configuration for a workflow type
   */
  async getBestConfiguration(
    workflowType: string,
    criteria: {
      metric?: 'confidence' | 'test_success_rate' | 'cost_effectiveness';
      min_tests?: number;
    } = {}
  ): Promise<AutoGeneratedConfig | null> {
    const { metric = 'confidence', min_tests = 3 } = criteria;

    let orderBy: string;
    switch (metric) {
      case 'confidence':
        orderBy = 'c.confidence_score DESC';
        break;
      case 'test_success_rate':
        orderBy = '(COUNT(*) FILTER (WHERE t.success = true))::float / COUNT(*) DESC';
        break;
      case 'cost_effectiveness':
        orderBy = 'AVG((t.performance_metrics->>\'accuracy\')::float / NULLIF((t.performance_metrics->>\'cost\')::float, 0)) DESC';
        break;
      default:
        orderBy = 'c.confidence_score DESC';
    }

    const result = await this.pool.query(
      `SELECT c.*, 
              COUNT(t.test_id) as test_count,
              AVG((t.performance_metrics->>\'execution_time\')::float) as avg_execution_time,
              AVG((t.performance_metrics->>\'cost\')::float) as avg_cost
       FROM auto_generated_configs c
       LEFT JOIN config_test_results t ON c.config_id = t.config_id
       WHERE c.workflow_type = $1
       GROUP BY c.config_id
       HAVING COUNT(t.test_id) >= $2
       ORDER BY ${orderBy}
       LIMIT 1`,
      [workflowType, min_tests]
    );

    return result.rows[0] || null;
  }

  /**
   * Get configuration by ID
   */
  async getConfig(configId: string): Promise<AutoGeneratedConfig> {
    const result = await this.pool.query(
      `SELECT * FROM auto_generated_configs WHERE config_id = $1`,
      [configId]
    );

    if (result.rows.length === 0) {
      throw new Error(`Configuration not found: ${configId}`);
    }

    return result.rows[0];
  }

  /**
   * Get test results for a configuration
   */
  async getTestResults(configId: string): Promise<ConfigTestResult[]> {
    const result = await this.pool.query(
      `SELECT * FROM config_test_results
       WHERE config_id = $1
       ORDER BY tested_at DESC`,
      [configId]
    );

    return result.rows;
  }

  /**
   * Get configuration statistics
   */
  async getConfigStatistics(workflowType?: string): Promise<any> {
    const whereClause = workflowType ? 'WHERE workflow_type = $1' : '';
    const params = workflowType ? [workflowType] : [];

    const result = await this.pool.query(
      `SELECT 
         workflow_type,
         COUNT(*) as total_configs,
         AVG(confidence_score) as avg_confidence,
         COUNT(*) FILTER (WHERE generation_method = 'pattern_based') as pattern_based_count,
         COUNT(*) FILTER (WHERE generation_method = 'ml_optimized') as ml_optimized_count
       FROM auto_generated_configs
       ${whereClause}
       GROUP BY workflow_type`,
      params
    );

    return result.rows;
  }
}
